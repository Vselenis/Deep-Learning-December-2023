{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a7c4f49",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "\n",
    "<body>\n",
    "\n",
    "<h1><center>BERT: Pre-training of Deep Bidirectional Transformers for\n",
    "Language Understanding\n",
    "</center></h1> \n",
    "<h4><center>Peer review</center></h4>\n",
    "\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd1bf8d",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748c7b67",
   "metadata": {},
   "source": [
    "The paper presents BERT (Bidirectional Encoder Representations from Transformers), a groundbreaking technique in Natural Language Processing (NLP) introduced by researchers at Google AI Language. BERT's release in late 2018 marked a significant advancement in the field due to its remarkable performance and ability to capture bidirectional contextual information in text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b2c7ea",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9b4852",
   "metadata": {},
   "source": [
    "BERT employs a multi-layer bidirectional Transformer encoder, allowing it to capture contextual information from both left and right contexts. This architecture enables the model to generate high-quality representations of text sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f48b76",
   "metadata": {},
   "source": [
    "## Key Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e6d3b8",
   "metadata": {},
   "source": [
    "The review outlines the core concepts underlying BERT's architecture and training methodology. BERT relies on a Transformer model, utilizing self-attention mechanisms to process input sequences. The input is preprocessed with token, segment, and positional embeddings to provide additional context for the model. BERT is pre-trained on two NLP tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). MLM involves predicting masked words within sentences, while NSP focuses on understanding the relationship between pairs of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af390c7c",
   "metadata": {},
   "source": [
    "## Pre-training and Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c08626",
   "metadata": {},
   "source": [
    "The model follows a two-step process: pre-training and fine-tuning. During pre-training, it learns representations of words by training on vast amounts of unlabeled text data using two unsupervised tasks: Masked Language Model (MLM) and Next Sentence Prediction (NSP). Fine-tuning involves adapting the pre-trained model to specific downstream tasks by adjusting task-specific input and output layers while keeping the pre-trained parameters fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7d9274",
   "metadata": {},
   "source": [
    "## Experimental Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5937c8fc",
   "metadata": {},
   "source": [
    "The study presents extensive experimental results on various NLP tasks, including the General Language Understanding Evaluation (GLUE) benchmark, Stanford Question Answering Dataset (SQuAD), and Situations With Adversarial Generations (SWAG) dataset. The model achieves state-of-the-art performance across these tasks, outperforming previous models by a significant margin.The study explores the impact of model size on task accuracy. Larger models consistently lead to improved performance across different datasets, demonstrating the scalability of the approach. Performance is compared with prior state-of-the-art methods, highlighting its superiority in terms of accuracy and robustness. Ablation experiments are conducted to assess the importance of different pre-training objectives and model configurations. The results demonstrate the effectiveness of BERT's bidirectional pre-training and the significance of tasks like NSP for improving performance on downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccc0c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7012f639",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"twitter/twitter_training.csv\", header=None, names=[\"id\", \"category\", \"sentiment\", \"text\"])\n",
    "test_data = pd.read_csv(\"twitter/twitter_validation.csv\", header=None, names=[\"id\", \"category\", \"sentiment\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcc20042",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna(subset=[\"text\"])\n",
    "test_data = test_data.dropna(subset=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e1c3896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id     category sentiment  \\\n",
       "0  2401  Borderlands  Positive   \n",
       "1  2401  Borderlands  Positive   \n",
       "2  2401  Borderlands  Positive   \n",
       "3  2401  Borderlands  Positive   \n",
       "4  2401  Borderlands  Positive   \n",
       "\n",
       "                                                text  \n",
       "0  im getting on borderlands and i will murder yo...  \n",
       "1  I am coming to the borders and I will kill you...  \n",
       "2  im getting on borderlands and i will kill you ...  \n",
       "3  im coming on borderlands and i will murder you...  \n",
       "4  im getting on borderlands 2 and i will murder ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27c8d8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>4891</td>\n",
       "      <td>GrandTheftAuto(GTA)</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>⭐️ Toronto is the arts and culture capital of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>4359</td>\n",
       "      <td>CS-GO</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2652</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Today sucked so it’s time to drink wine n play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>8069</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Bought a fraction of Microsoft today. Small wins.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>6960</td>\n",
       "      <td>johnson&amp;johnson</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Johnson &amp; Johnson to stop selling talc baby po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             category   sentiment  \\\n",
       "995  4891  GrandTheftAuto(GTA)  Irrelevant   \n",
       "996  4359                CS-GO  Irrelevant   \n",
       "997  2652          Borderlands    Positive   \n",
       "998  8069            Microsoft    Positive   \n",
       "999  6960      johnson&johnson     Neutral   \n",
       "\n",
       "                                                  text  \n",
       "995  ⭐️ Toronto is the arts and culture capital of ...  \n",
       "996  tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...  \n",
       "997  Today sucked so it’s time to drink wine n play...  \n",
       "998  Bought a fraction of Microsoft today. Small wins.  \n",
       "999  Johnson & Johnson to stop selling talc baby po...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f432e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_texts = train_data[\"text\"].tolist()\n",
    "test_texts = test_data[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b3039fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 testing texts:\n",
      "Text 1: I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tom’s great auntie as ‘Hayley can’t get out of bed’ and told to his grandma, who now thinks I’m a lazy, terrible person 🤣\n",
      "Text 2: BBC News - Amazon boss Jeff Bezos rejects claims company acted like a 'drug dealer' bbc.co.uk/news/av/busine…\n",
      "Text 3: @Microsoft Why do I pay for WORD when it functions so poorly on my @SamsungUS Chromebook? 🙄\n",
      "Text 4: CSGO matchmaking is so full of closet hacking, it's a truly awful game.\n",
      "Text 5: Now the President is slapping Americans in the face that he really did commit an unlawful act after his  acquittal! From Discover on Google vanityfair.com/news/2020/02/t…\n"
     ]
    }
   ],
   "source": [
    "print(\"First 5 testing texts:\")\n",
    "for i, text in enumerate(test_texts[:5]):\n",
    "    print(f\"Text {i + 1}: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b13b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "test_encodings = tokenizer(test_texts, return_tensors=\"tf\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73609863",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[\"sentiment\"])\n",
    "test_labels = label_encoder.transform(test_data[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01286e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "input_ids = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
    "output = bert_model([input_ids, attention_mask])[1]\n",
    "outputs = tf.keras.layers.Dense(len(label_encoder.classes_), activation=\"softmax\")(output)\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a76c02e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, None                                               \n",
      "                                , 768),                                                           \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4)            3076        ['tf_bert_model[0][1]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a0f7f",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ea7626",
   "metadata": {},
   "source": [
    "Overall, the review acknowledges the significance of advancing NLP research and applications. Effectiveness, ease of use, and state-of-the-art performance make it a valuable tool for researchers and practitioners in the field. The paper's clear explanations and practical guidance on fine-tuning contribute to its widespread adoption and success across diverse language understanding tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5d66d4",
   "metadata": {},
   "source": [
    "## References "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241b9a55",
   "metadata": {},
   "source": [
    "* [BERT - arxiv.org](https://arxiv.org/abs/1810.04805)\n",
    "* [Fine-tuning a BERT model - tensorflow.org](https://www.tensorflow.org/tfmodels/nlp/fine_tune_bert)\n",
    "* [BERT for Text Classification - github.com](https://github.com/shrikantnaidu/BERT-for-Text-Classification-with-TensorFlow/blob/main/Fine_Tune_BERT_for_Text_Classification_with_TensorFlow.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
